{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from pyspark.sql import SparkSession, Window\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"external_datasources_common_operations\")\\\n",
    ".config(\"spark.jars\", \"/opt/bitnami/spark/connectors/jars/postgresql-42.6.0.jar\") \\\n",
    ".config(\"spark.driver.extraClassPath\", \"/opt/bitnami/spark/connectors/jars/postgresql-42.6.0.jar\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Drop tables postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = ['airport', 'delay']\n",
    "connection_params = {\n",
    "    \"host\": \"host.docker.internal\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "}\n",
    "with psycopg2.connect(**connection_params) as conn, conn.cursor() as cursor:\n",
    "    for table in table_names:\n",
    "        sql_query = f\"DROP TABLE IF EXISTS {table}\"\n",
    "        cursor.execute(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Create tables postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_params = {\n",
    "    \"host\": \"host.docker.internal\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "}\n",
    "with psycopg2.connect(**connection_params) as conn, conn.cursor() as cursor:\n",
    "    cursor.execute(\"\"\"CREATE TABLE airport (\n",
    "                        \"City\" TEXT,\n",
    "                        \"State\" TEXT,\n",
    "                        \"Country\" TEXT,\n",
    "                        \"IATA\" TEXT\n",
    "                        )\"\"\")\n",
    "    cursor.execute(\"\"\"CREATE TABLE delay (\n",
    "                        date TEXT,\n",
    "                        delay INT,\n",
    "                        distance INT,\n",
    "                        origin TEXT,\n",
    "                        destination TEXT\n",
    "                        )\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Load data postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport = spark.read.format('csv').option('header', 'true').option(\"inferSchema\", \"true\").option('sep','\\t').load(\"/opt/bitnami/spark/custom_data/chapter5/airport-codes-na.txt\")\n",
    "delay = spark.read.format('csv').option('header', 'true').option(\"inferSchema\", \"true\").load(\"/opt/bitnami/spark/custom_data/chapter5/departuredelays.csv\")\n",
    "\n",
    "airport.write.format(\"jdbc\")\\\n",
    ".option('url', 'jdbc:postgresql://host.docker.internal:5432/postgres')\\\n",
    ".option('dbtable', \"airport\")\\\n",
    ".option('user', 'postgres')\\\n",
    ".option('password', 'postgres')\\\n",
    ".mode('append').save()\n",
    "\n",
    "delay.write.format(\"jdbc\")\\\n",
    ".option('url', 'jdbc:postgresql://host.docker.internal:5432/postgres')\\\n",
    ".option('dbtable', \"delay\")\\\n",
    ".option('user', 'postgres')\\\n",
    ".option('password', 'postgres')\\\n",
    ".mode('append').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Read data postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport = spark.read.format(\"jdbc\")\\\n",
    ".option('url', 'jdbc:postgresql://host.docker.internal:5432/postgres')\\\n",
    ".option('dbtable', \"airport\")\\\n",
    ".option('user', 'postgres')\\\n",
    ".option('password', 'postgres')\\\n",
    ".load()\n",
    "\n",
    "delay = spark.read.format(\"jdbc\")\\\n",
    ".option('url', 'jdbc:postgresql://host.docker.internal:5432/postgres')\\\n",
    ".option('dbtable', \"delay\")\\\n",
    ".option('user', 'postgres')\\\n",
    ".option('password', 'postgres')\\\n",
    ".load()\n",
    "\n",
    "# .option('numPartitions', 10)  # Performs query in parallel, start with multiple of the number of spark workers.\n",
    "# This option can work alone without the below options. The jdbc will determine the partitions automatically. \n",
    "# .option('partitionColumn', 'your_partition_column')  # Specify the column to partition on\n",
    "# .option('lowerBound', 1)  # Specify the lower bound value for the partitioning\n",
    "# .option('upperBound', 100)  # Specify the upper bound value for the partitioning\n",
    "# .option('query', \"SELECT * FROM delay limit 10\") # To perform custom sql to extract data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Common operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "airport.createOrReplaceTempView(\"airports\")\n",
    "delay.createOrReplaceTempView(\"departureDelays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = delay.filter(F.expr(\"\"\"origin=='SEA' and destination=='SFO' and delay>0 and date like '1220%'\"\"\"))\n",
    "foo.createOrReplaceTempView(\"foo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = delay.union(foo)\n",
    "bar.filter(F.expr(\"\"\"origin=='SEA' and destination=='SFO' and delay>0 and date like '1220%'\"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "foo.join(airport, airport.IATA==foo.origin).select('City', 'State', 'date', 'distance', 'destination').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy('origin').orderBy(F.desc('delay'))\n",
    "delay.withColumn('ranking', F.dense_rank().over(w)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "delay_pivot = delay.withColumn('month', F.expr(\"CAST(SUBSTRING(date,0,1) as int)\")).filter(F.expr(\"origin='SEA'\")).select(\"destination\", \"month\", \"delay\")\n",
    "delay_pivot.groupBy(\"destination\").pivot('month').sum('delay').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
