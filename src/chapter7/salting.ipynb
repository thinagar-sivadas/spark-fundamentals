{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"salting\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"3\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Uniform dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniform = spark.createDataFrame([i for i in range(1000000)], IntegerType())\n",
    "df_uniform.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_uniform\n",
    "    .withColumn(\"partition\", F.spark_partition_id())\n",
    "    .groupBy(\"partition\")\n",
    "    .count()\n",
    "    .orderBy(\"partition\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Skewed dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = spark.createDataFrame([0]*999990, IntegerType()).repartition(1)\n",
    "df1 = spark.createDataFrame([1]*15, IntegerType()).repartition(1)\n",
    "df2 = spark.createDataFrame([2]*10, IntegerType()).repartition(1)\n",
    "df3 = spark.createDataFrame([3]*5, IntegerType()).repartition(1)\n",
    "df_skew = df0.union(df1).union(df2).union(df3)\n",
    "df_skew.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_skew\n",
    "    .withColumn(\"partition\", F.spark_partition_id())\n",
    "    .groupBy(\"partition\")\n",
    "    .count()\n",
    "    .orderBy(\"partition\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# Skewed join dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined_c1 = df_skew.join(df_uniform,'value', 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_joined_c1\n",
    "    .withColumn(\"partition\", F.spark_partition_id())\n",
    "    .groupBy(\"partition\")\n",
    "    .count()\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Simulating uniform distribution through salting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "SALT_NUMBER = int(spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
    "SALT_NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skew = df_skew.withColumn(\"salt\", (F.rand()*SALT_NUMBER).cast(\"int\"))\n",
    "df_skew.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniform = (\n",
    "    df_uniform\n",
    "    .withColumn(\"salt_values\", F.array([F.lit(i) for i in range(SALT_NUMBER)]))\n",
    "    .withColumn(\"salt\", F.explode(F.col(\"salt_values\")))\n",
    ")\n",
    "df_uniform.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df_skew.join(df_uniform,['value', 'salt'], 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_joined\n",
    "    .withColumn(\"partition\", F.spark_partition_id())\n",
    "    .groupBy(\"value\",\"partition\")\n",
    "    .count()\n",
    "    .orderBy(\"value\",\"partition\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Salting in aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_skew\n",
    "    .groupBy(\"value\", \"salt\")\n",
    "    .agg(F.count(\"value\").alias(\"count\"))\n",
    "    .groupBy(\"value\")\n",
    "    .agg(F.sum(\"count\").alias(\"count\"))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
